{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging Practical Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "no_solution"
    ]
   },
   "source": [
    "This part was produced using [Jupyter](http://jupyter.org).  \n",
    "If you are used to it, you can [download the corresponding notebook code from here](TP3-PoSTagging.ipynb). If not, no problem at all, this is not mandatory at all: simply proceed as usual in your favorite Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this practical session, our goals are to:\n",
    "\n",
    "* better understand the role and mechanisms behind PoS tagging;\n",
    "* explore applications of PoS tagging such as dealing with ambiguity or vocabulary reduction;\n",
    "* get accustomed to the Viterbi algorithm through a concrete example.\n",
    "\n",
    "This practical session is making use of the NLTK. Please refer to [this part of first practical session](TP1.php#nltk) for a setup.\n",
    "\n",
    "## 1. Parts of Speech and Ambiguity\n",
    "\n",
    "For this exercise, we will be using the basic functionality of the built-in PoS tagger from NLTK. All you need to know for this part can be found in [section 1  of chapter 5 of the NLTK book](http://www.nltk.org/book/ch05.html#using-a-tagger).\n",
    "\n",
    "1. Search the web for \"ambiguous headlines\", to find such gems as: \"_British Left Waffles on Falkland Islands_\", and \"_Juvenile Court to Try Shooting Defendant_\". [That website (link)](http://www.ling.upenn.edu/~beatrice/humor/headlines.html) has some good ones.\n",
    "1. Manually tag (by yourself) these headlines.\n",
    "   Do you think the information about part-of-speech tags would help a first reading?\n",
    "1. Use the built-in NLTK PoS tagger on the headlines.\n",
    "   Does it find the correct tags in these cases?\n",
    "\n",
    "**Note:** NLTK provides documentation for each tag, which can be queried using the tag, e.g.\n",
    "\n",
    "    nltk.help.upenn_tagset('RB')\n",
    "\n",
    "or some regular expression, e.g.\n",
    "\n",
    "    nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this practical session, you need a few nltk ressources. If not done yet (from [the first practical session](http://localhost:8888/notebooks/03-Tagging/TP1.php#nltk), please do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/my_venvs/cs_431_env/lib/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/my_venvs/cs_431_env/lib/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/my_venvs/cs_431_env/lib/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/my_venvs/cs_431_env/lib/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/my_venvs/cs_431_env/lib/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if not downloaded yet (see first practical session)\n",
    "import os\n",
    "import nltk\n",
    "install_path = os.path.join(os.path.expanduser(\"~\"), 'my_venvs', 'cs_431_env', 'lib', 'nltk_data')\n",
    "print(install_path)\n",
    "\n",
    "nltk.download('brown', download_dir = install_path)\n",
    "nltk.download('universal_tagset', download_dir = install_path)\n",
    "nltk.download('wordnet', download_dir = install_path)\n",
    "nltk.download('tagsets', download_dir = install_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/my_venvs/cs_431_env/lib/nltk_data/\n"
     ]
    }
   ],
   "source": [
    "query_path = '/home/my_venvs/cs_431_env/lib/nltk_data/'\n",
    "print(query_path)\n",
    "\n",
    "nltk.data.path.append(query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it might also be usefull to already import some symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# some headline\n",
    "headline = \"British Left Waffles on Falkland Islands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**to do:** complete the above code to tag your headline phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and then tag the headline\n",
    "tagged = nltk.pos_tag( nltk.word_tokenize( headline ) );\n",
    "print( tagged );\n",
    " \n",
    "# some help, maybe...\n",
    "for (word, tag) in tagged:\n",
    "    nltk.help.upenn_tagset( tag )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adverbs\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "Nouns\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "None\n",
      "Verbs\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n",
      "Prepositions\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# All possible tags in NLTK: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "print('Adverbs')\n",
    "print(nltk.help.upenn_tagset('RB'))\n",
    "print('Nouns')\n",
    "print(nltk.help.upenn_tagset('NN.*'))\n",
    "print('Verbs')\n",
    "print(nltk.help.upenn_tagset('V.*'))\n",
    "print('Prepositions')\n",
    "print(nltk.help.upenn_tagset('RP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging of the headline\n",
    "#### Headline: \"British Left Waffles on Falkland Islands\"\n",
    "\n",
    "Manual tagging: British`Noun`, Left`Verb`, Waffles`Noun`, On`Prep`, Falkland`Noun`, Islands`Noun` <br/>\n",
    "NLTK tagging: British`NNP`, Left`VBD`, Waffles`NNS`, On`RP`, Falkland`NNP`, Islands`NNS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vocabulary size and variability reduction\n",
    "\n",
    "In this exercise, we will be exploring the interplay between PoS tagging, lemmatization, and vocabulary reduction.\n",
    "\n",
    "Imagine you have a large corpus and a large number of queries of the form “_Give me all documents in which all of the words x, y and  z appear_”. One way to answer such queries efficiently is to index the documents in the corpus: for instance implement a function called `index()` for which `index(document, word)` returns non-zero if and only if word occurs in document at least once.\n",
    "\n",
    "One way to implement this kind of index is with a matrix in which each line corresponds to a document and each column corresponds to a word. When the query above is executed, only the lines for which the entries on the x, y and z columns are simultaneously not zero are returned.\n",
    "\n",
    "The size of this matrix is number of documents × number of distinct words that appear in the corpus. Clearly, the smaller the matrix, the faster the queries are executed. Ideally, we would like to reduce the number of words while not losing the ability to answer queries. One way to do this is to eliminate morphological variability — for instance, map both “_goes_” and “_went_” to the same column “_go_”. This is called \"_lemmatization_\". Part-of-speech tagging can help lemmatization so as not to confuse similar surface forms with different roles (e.g. not confuse \"_can_\" as in \"_a can of beer_\" as opposed to \"_I can do it_\".).\n",
    "\n",
    "* To begin, we will need a corpus to work with. Let's make use of the [Brown Corpus](http://en.wikipedia.org/wiki/Brown_Corpus). Load the list of all word-tag tuples from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brown_tagged = brown.tagged_words(tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see what you got; e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the “[universal](http://www.nltk.org/book/ch05.html#tab-universal-tagset)” tagset because the original Brown tagset is somewhat unwieldy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we want to see how many distinct words appear in the corpus. Variations of the same word should be counted separately, e.g. \"_go_\" and \"_went_\" shall be counted separately here, but case should be ignored. One way to do this is to use the NLTK-supplied [`FreqDist` class](http://www.nltk.org/_modules/nltk/probability.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fdist_before = nltk.FreqDist(word.lower() for (word, tag) in brown_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This builds a frequency distribution from the words in the corpus (have a look!).\n",
    "To see how many distinct words are in the distribution, simply take the length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49815"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist_before) ## should be 49815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now run the [WordNet Lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html) on the tagged corpus.\n",
    "  Note that this lemmatizer needs information about the part of speech of a token in order to correctly lemmatize it.\n",
    "  One way of lemmatizing the entire corpus is to iterate through the list of (token, tag) pairs — `brown_tagged` variable above —, lemmatize each token and add the lemma to a separate list.\n",
    "  For the part-of-speech format needed for lemmatization, you might find the following function useful (do not forget to `import wordnet from nltk.corpus`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converts a pos tag from the 'universal' format to wordnet format \n",
    "def get_wordnet_pos(universal_tag):\n",
    "    if universal_tag == 'VERB':\n",
    "        return wordnet.VERB\n",
    "    elif universal_tag == 'ADJ':\n",
    "        return wordnet.ADJ\n",
    "    elif universal_tag == 'ADV':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count the distinct lemmatized words, for example using FreqDist. Do not forget to convert everything to lowercase.\n",
    "  What percentage reduction did you obtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "no_solution"
    ]
   },
   "source": [
    "[**to do:** write Python code to perform lemmatization of the Brown corpus and answer the above question.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqDist({'the': 69971, ',': 58334, '.': 49346, 'of': 36412, 'and': 28853, 'to': 26158, 'a': 23195, 'in': 21337, 'that': 10594, 'is': 10109, ...})\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# just to take a peek at the FreqDist\n",
    "print(fdist_before.pprint(maxlen = 10))\n",
    "\n",
    "# start lemmatising the corpus and add lemma to seperate list\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Let this be a list of tuples with word, tag\n",
    "lemmatised_dict = {}\n",
    "\n",
    "for word, tag in brown_tagged:\n",
    "    \n",
    "    lemmatised_wrd = lemmatiser.lemmatize(word.lower(), get_wordnet_pos(tag))\n",
    "    \n",
    "    # Store lemmatised word in a dict for easy access later\n",
    "    cur_stored_item = lemmatised_dict.get(lemmatised_wrd, {})\n",
    "    \n",
    "    # if lemmatised word is already in dictionary, increase count by 1\n",
    "    if bool(cur_stored_item):\n",
    "        cur_stored_item['cnt'] += 1\n",
    "    \n",
    "    # else, initialise a new dict with count of 1 and corresponding tag\n",
    "    else:\n",
    "        cur_stored_item['cnt'] = 1\n",
    "        \n",
    "    lemmatised_dict[lemmatised_wrd] = cur_stored_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of distinct words: 49815\n",
      "No. of distinct lemmatised words: 39358\n",
      "Percentage reduction: 0.2099166917595102\n"
     ]
    }
   ],
   "source": [
    "print(f'No. of distinct words: {len(fdist_before)}')\n",
    "print(f'No. of distinct lemmatised words: {len(lemmatised_dict)}')\n",
    "print(f'Percentage reduction: {(len(fdist_before) - len(lemmatised_dict))/len(fdist_before)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatised word of goes: go\n",
      "Lemmatised word of run(V): run\n",
      "Lemmatised word of run(N): run\n"
     ]
    }
   ],
   "source": [
    "lemmatised_word1 = lemmatiser.lemmatize('goes', wordnet.VERB)\n",
    "print(f'Lemmatised word of goes: {lemmatised_word1}')\n",
    "\n",
    "lemmatised_word2 = lemmatiser.lemmatize('run', wordnet.VERB)\n",
    "print(f'Lemmatised word of run(V): {lemmatised_word2}')\n",
    "\n",
    "lemmatised_word3 = lemmatiser.lemmatize('run', wordnet.NOUN)\n",
    "print(f'Lemmatised word of run(N): {lemmatised_word3}')\n",
    "\n",
    "# Note: The same word with different PoS tag may be lemmatised to the same word. Not that meaningful to\n",
    "# store PoS tag in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram tagging\n",
    "### 3.1 Unigram tagger\n",
    "\n",
    "In this exercise, we will see how adding context can improve the performance of automatic part-of-speech tagging. From the NLTK point of view, everything you need to know can be found in [section 5 of chapter 5 of the book](http://www.nltk.org/book/ch05.html#n-gram-tagging).\n",
    "\n",
    "We will begin with a simple unigram tagger and build it up to a slightly more complex tagger. Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token.\n",
    "\n",
    "* The first thing we want to do is to consider a corpus to work with and separate it into a training set and a test set. We recommend the Brown corpus again, but this time it is a good idea to restrict it to one or two of its categories, otherwise the experiments would be too slow (we choose news and fiction categories below):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus: 8872 sentences\n",
      "Size of training set: 7984 sentencces\n"
     ]
    }
   ],
   "source": [
    "#import the tagged and untagged sentences\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['news', 'fiction'])\n",
    "print(\"Size of corpus: {0} sentences\".format(len(brown_tagged_sents)))\n",
    "\n",
    "# split the sentences into training and test sets\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "print(\"Size of training set: {0} sentencces\".format(size))\n",
    "\n",
    "# Tagged test sentences (used as reference)\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "# Untagged test sentences\n",
    "raw_sents = brown.sents(categories=['news', 'fiction'])[size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train a unigram tagger (on the training set of sentences) and run it on some of the sentences from `raw_sents`. Observe that some words are not assigned a tag.\n",
    "  Why not?\n",
    "* What is the accuracy of the tagger on the test set (the [`evaluate()` function](http://www.nltk.org/api/nltk.tag.html#nltk.tag.api.TaggerI.evaluate) may come in handy here)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "no_solution"
    ]
   },
   "source": [
    "[**to do:** write Python code to perform unigram tagger learning on the Brown corpus and answer the above questions.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('To', 'TO'), ('Mark', 'NP'), (':', ':'), ('``', '``'), ('Please', 'VB'), ('give', 'VB'), ('my', 'PP$'), ('regards', None), ('to', 'TO'), ('Myra', None), (\"''\", \"''\"), ('.', '.')]\n",
      "[('``', '``'), ('Um', None), (\"''\", \"''\"), (',', ','), ('said', 'VBD'), ('the', 'AT'), ('old', 'JJ'), ('lady', 'NN'), (',', ','), ('and', 'CC'), ('brought', 'VBD'), ('her', 'PP$'), ('eyes', 'NNS'), ('down', 'RP'), ('to', 'TO'), ('the', 'AT'), ('tray', None), ('.', '.')]\n",
      "Accuracy of unigram tagger: 0.8502823106037104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/3774325492.py:12: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of unigram tagger: {unigram_tagger.evaluate(test_sents)}')\n"
     ]
    }
   ],
   "source": [
    "# Train a unigram tagger \n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "\n",
    "# Test out trained tagger on some random test sentences\n",
    "print(unigram_tagger.tag(raw_sents[0]))\n",
    "print(unigram_tagger.tag(raw_sents[10]))\n",
    "\n",
    "# Reason why some words are not assigned a tag: Training data is not large enough and model has not seen some\n",
    "# words before. Hence,it is not able to tag words it has not seen before\n",
    "\n",
    "# Accuracy of tagger\n",
    "print(f'Accuracy of unigram tagger: {unigram_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Guesser\n",
    "\n",
    "An easy way to mitigate the problem of some words not being assigned tags is to have the unigram tagger fall back on a guesser when it does not know what tag to assign to a word.\n",
    "\n",
    "As its name suggests, a guesser is a PoS Tagger that assigns a tag to any token (be it a correct word or not). The use of a guesser as a fallback can improve the robustness of the PoS tagging system (i.e. the system will associate a PoS tag to every word, even if it was not seen beforehand, or if it is not in the lexicon).\n",
    "Another scenario in which a guesser could be helpful is in the initialization phase of the Brill tagging algorithm. In addition, the guesser’s standalone performance could be used as a baseline, when evaluating more complex taggers.\n",
    "\n",
    "For instance, perhaps the simplest guesser is the one that just assigns the same tag to every word it encounters. This is called a [“_default tagger_”](http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.DefaultTagger) in NLTK.\n",
    "\n",
    "* Instantiate a default tagger and use it to tag the test sentences (`test_sents`) defined in the previous section.\n",
    "  Do not combine it yet with other taggers at this point.\n",
    "  Report the accuracy. (Feel free to look at the examples in [sections 4](http://www.nltk.org/book/ch05.html#automatic-tagging)  [and 5](http://www.nltk.org/book/ch05.html#n-gram-tagging)  from chapter 5 of the NLTK book.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of default tagger: 0.11230377861884966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/2981797804.py:9: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of default tagger: {default_tagger.evaluate(test_sents)}')\n"
     ]
    }
   ],
   "source": [
    "# Get the most frequent tag in our train dataset\n",
    "from itertools import chain\n",
    "\n",
    "words_and_tags = list(chain.from_iterable(train_sents))\n",
    "tags = [tag for (word,tag) in words_and_tags]\n",
    "most_freq_tag = nltk.FreqDist(tags).max()\n",
    "\n",
    "default_tagger = nltk.DefaultTagger(most_freq_tag)\n",
    "print(f'Accuracy of default tagger: {default_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then use the default tagger as a fallback for a Unigram tagger. Report the change in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/3834158350.py:17: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of unigram tagger: {unigram_with_default_tagger.evaluate(test_sents)}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger: 0.868834150276106\n",
      "Accuracy of bigram tagger: 0.866848669107154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/3834158350.py:23: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of bigram tagger: {bigram_tagger.evaluate(test_sents)}')\n"
     ]
    }
   ],
   "source": [
    "# Get most frequent tag in corpus\n",
    "from itertools import chain\n",
    "\n",
    "words_and_tags = list(chain.from_iterable(train_sents))\n",
    "tags = [tag for (word,tag) in words_and_tags]\n",
    "most_freq_tag = nltk.FreqDist(tags).max()\n",
    "\n",
    "# Instantiate the default tagger\n",
    "default_tagger = nltk.DefaultTagger(most_freq_tag)\n",
    "\n",
    "# Instantiate the unigram tagger which uses the default tagger as a fallback\n",
    "unigram_with_default_tagger = nltk.UnigramTagger(train_sents, backoff = default_tagger)\n",
    "\n",
    "# Report the accuracy\n",
    "\n",
    "# Accuracy of tagger\n",
    "print(f'Accuracy of unigram tagger: {unigram_with_default_tagger.evaluate(test_sents)}')\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Extra: Instantiate a bigram which uses unigram as a backoff\n",
    "bigram_tagger = nltk.BigramTagger(train_sents, backoff = unigram_tagger)\n",
    "# Accuracy of tagger\n",
    "print(f'Accuracy of bigram tagger: {bigram_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How would you build a better guesser, while still not relying on any further training data?\n",
    "\n",
    "A: Make use of longer n-grams where n > 1. This allows for more dependencies between the words to be captured and lead to better accuracy while not requiring more training data. Also, default taggers/ simpler n-gram models can be used as the fall back when the lead model is not able to assign a tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One step further than the default tagger could be to assign tags to tokens on the basis of matching patterns. In other words, the form of the token is considered when guessing its part-of-speech. For example, it might guess that any word ending in “ed” is the past participle of a verb, and any word ending with “'s” is a possessive noun. This is called a [“_regular expression tagger_”](http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.RegexpTagger) and is already available in NLTK.\n",
    "\n",
    "* Instantiate a regular expression tagger and use it to tag the test sentences. You could use the same rules provided in the link above, or define some other rules of your own.\n",
    "  Do not combine it yet with other taggers at this point.\n",
    "  Report the accuracy.\n",
    "* Now use the regular expression tagger as a fallback for a Unigram tagger.\n",
    "  Report the change in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/1215325866.py:15: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of Reg Expression tagger: {regexp_tagger.evaluate(test_sents)}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Reg Expression tagger: 0.18303654526276603\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a regular expression tagger\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),                # gerunds\n",
    "        (r'.*ed$', 'VBD'),                 # simple past\n",
    "        (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),                # modals\n",
    "        (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                  # plural nouns\n",
    "        (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                      # nouns (default)\n",
    "    ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "print(f'Accuracy of Reg Expression tagger: {regexp_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Unigram tagger: 0.8824222870261215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/1913435978.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of Unigram tagger: {unigram_with_regexp_tagger.evaluate(test_sents)}')\n"
     ]
    }
   ],
   "source": [
    "# Use egexp_tagger as a fallback for unigram tagger\n",
    "\n",
    "unigram_with_regexp_tagger = nltk.UnigramTagger(train_sents, backoff = regexp_tagger)\n",
    "print(f'Accuracy of Unigram tagger: {unigram_with_regexp_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bigrams and More\n",
    "\n",
    "When performing PoS tagging using a unigram-based tagger, the larger context in which the words appear is ignored. Unigram taggers consider only one item of context, in isolation. This poses a significant limitation on any tagger in this class: the most that it can do is to assign the a priori most likely tag to a given token. In a sense, the tag of each word is decided before it is placed in a sentence.  Therefore, the word “_object_” would be assigned the same part of speech every time, regardless if it appears as “_the object_” (should be a noun) or as “_to object_” (should be a verb).  \n",
    "\n",
    "An n-gram tagger picks the tag that is most likely in the given context of size n. An instance of an n-gram tagger is the bigram tagger, which considers groups of two tokens when deciding on the parts-of-speech. For example, we would expect a bigram tagger to find the correct tags for the two occurrences of the word “_objec_t” in the following sentence (as opposed to a unigram tagger):\n",
    "\n",
    "“_I did not object to the object_”\n",
    "\n",
    "Indeed, a NLTK unigram tagger trained on the Brown corpus yields: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/my_venvs/cs_431_env/lib/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/my_venvs/cs_431_env/lib/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "install_path = os.path.join(os.path.expanduser(\"~\"), 'my_venvs', 'cs_431_env', 'lib', 'nltk_data')\n",
    "print(install_path)\n",
    "\n",
    "nltk.download('punkt', download_dir = install_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PPSS'), ('did', 'DOD'), ('not', '*'), ('object', 'VB'), ('to', 'TO'), ('the', 'AT'), ('object', 'VB')]\n",
      "[('I', 'PPSS'), ('did', 'DOD'), ('not', '*'), ('object', 'VB'), ('to', 'TO'), ('the', 'AT'), ('object', 'VB')]\n",
      "[('I', 'PPSS'), ('did', 'DOD'), ('not', '*'), ('object', 'VB'), ('to', 'TO'), ('the', 'AT'), ('object', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I did not object to the object\"\n",
    "tokens =  nltk.word_tokenize(sentence)\n",
    "\n",
    "print(unigram_tagger.tag(tokens))\n",
    "print(unigram_with_default_tagger.tag(tokens))\n",
    "print(unigram_with_regexp_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No way to have a different tag for the two \"_object_\" words.\n",
    "\n",
    "* Train a bigram tagger alone on the training set (`train_sents`) and check it in the above sentence (weird result expected, see below).\n",
    "* Then, find its accuracy on the test set (`test_sents`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PPSS'), ('did', 'DOD'), ('not', '*'), ('object', 'VB'), ('to', 'TO'), ('the', None), ('object', None)]\n",
      "Accuracy of bigram tagger: 0.19631445058013278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/68309471.py:7: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of bigram tagger: {bigram_tagger.evaluate(test_sents)}')\n"
     ]
    }
   ],
   "source": [
    "# Tag the sentence using bigram tagger and see the output\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "\n",
    "print(bigram_tagger.tag(tokens))\n",
    "\n",
    "# Find out the accuracy on the test set\n",
    "print(f'Accuracy of bigram tagger: {bigram_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have probably found a surprising result on our sample sentence and very low accuracy for the bigram tagger, when run alone. If you run the trained bigram tagger on a sentence it has not seen during training (e.g. bigram.tag(raw_sents[1]), you will notice that a majority of words will be tagged as “None”. When the bigram tagger encounters a new word, it is unable to assign a tag. It will also be unable to tag the following word — even if it was seen during training — because it was never seen preceded by a “None” tag. From this point on, all the words up to the end of the sentence will be tagged as “None” for the same reason. This explains the low accuracy of the bigram tagger when run alone.\n",
    "\n",
    "The problem that arises when using n-grams is that once we pass to larger values of n, more data is needed for training, as the specificity of the context increases. In other words, there are many more possible bigrams than unigrams and thus the word/tag combinations that we need to consider when tagging are less likely to have appeared in the training set, as n increases. This is known as the sparse data problem, and is quite pervasive in NLP.\n",
    "\n",
    "This means that, when using n-gram taggers, we encounter the following tradeoff: having n large means more accurate predictions, at the cost of having to provide more training data; having a smaller n, leads to not so accurate predictions, but less necessary training data.\n",
    "\n",
    "In order to overcome this limitation, we can train a bigram tagger that falls back on our “unigram+default” or “unigram+regexp” taggers defined in the previous section.\n",
    "\n",
    "* Try this out now (an example for a bigram tagger with fallback on “unigram+default” can be found in the book, [section 5.5.4 “_Combining Taggers_”](http://www.nltk.org/book/ch05.html#combining-taggers)). Does this improve performance?\n",
    "* What about adding a trigram tagger that falls back on the “bigram+unigram+default” or on “bigram+unigram+regexp”?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/1075774205.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of bigram tagger with unigram+default fallback: {bigram_tagger.evaluate(test_sents)}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger with unigram+default fallback: 0.8848420922007818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/1075774205.py:7: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of trigram tagger with bigram+unigram+default fallback: {trigram_tagger.evaluate(test_sents)}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger with bigram+unigram+default fallback: 0.8845939070546628\n",
      "Accuracy of trigram tagger with bigram+unigram+default fallback: 0.898492275237327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/1075774205.py:12: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f'Accuracy of trigram tagger with bigram+unigram+default fallback: {trigram_tagger.evaluate(test_sents)}')\n"
     ]
    }
   ],
   "source": [
    "# Bigram tagger with unigram + default\n",
    "bigram_tagger = nltk.BigramTagger(train_sents, backoff = unigram_with_default_tagger)\n",
    "print(f'Accuracy of bigram tagger with unigram+default fallback: {bigram_tagger.evaluate(test_sents)}')\n",
    "\n",
    "# Trigram tagger with bigram + unigram + default\n",
    "trigram_tagger = nltk.TrigramTagger(train_sents, backoff = bigram_tagger)\n",
    "print(f'Accuracy of trigram tagger with bigram+unigram+default fallback: {trigram_tagger.evaluate(test_sents)}')\n",
    "\n",
    "# Trigram tagger with trigram + bigram + unigram + regexp\n",
    "bigram_tagger = nltk.BigramTagger(train_sents, backoff = unigram_with_regexp_tagger)\n",
    "trigram_tagger = nltk.TrigramTagger(train_sents, backoff = bigram_tagger)\n",
    "print(f'Accuracy of trigram tagger with bigram+unigram+default fallback: {trigram_tagger.evaluate(test_sents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Viterbi Algorithm\n",
    "\n",
    "Code the Viterbi algorithm (in your most favorite programming language) and apply it to the toy example provided on slide 8 of the lecture on HMM: two symbols (H,T), two states (the two coins),  \n",
    "A = [[ 0.4 0.6 ]; [0.9 0.1 ]],  \n",
    "B = [[ 0.49 0.51 ]; [ 0.85 0.15 ]],  \n",
    "I = [ 0.5 0.5 ].\n",
    "\n",
    "What is (one of) the most probable sequence of states corresponding to the observation `HTTHTTHHTTHTTTHHTHHTTHTTTTHTHHTHTHHTTTH`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "solns = ['211211121121111211211211112112121121112',\n",
    "'211211121121111211211211112112121211112',\n",
    "'211211121121111211211211112121121121112',\n",
    "'211211121121111211211211112121121211112',\n",
    "'211211121121111212111211112112121121112',\n",
    "'211211121121111212111211112112121211112',\n",
    "'211211121121111212111211112121121121112',\n",
    "'211211121121111212111211112121121211112',\n",
    "'211211121121112111211211112112121121112',\n",
    "'211211121121112111211211112112121211112',\n",
    "'211211121121112111211211112121121121112',\n",
    "'211211121121112111211211112121121211112',\n",
    "'211211121121112112111211112112121121112',\n",
    "'211211121121112112111211112112121211112',\n",
    "'211211121121112112111211112121121121112',\n",
    "'211211121121112112111211112121121211112',\n",
    "'211211211121111211211211112112121121112',\n",
    "'211211211121111211211211112112121211112',\n",
    "'211211211121111211211211112121121121112',\n",
    "'211211211121111211211211112121121211112',\n",
    "'211211211121111212111211112112121121112',\n",
    "'211211211121111212111211112112121211112',\n",
    "'211211211121111212111211112121121121112',\n",
    "'211211211121111212111211112121121211112',\n",
    "'211211211121112111211211112112121121112',\n",
    "'211211211121112111211211112112121211112',\n",
    "'211211211121112111211211112121121121112',\n",
    "'211211211121112111211211112121121211112',\n",
    "'211211211121112112111211112112121121112',\n",
    "'211211211121112112111211112112121211112',\n",
    "'211211211121112112111211112121121121112',\n",
    "'211211211121112112111211112121121211112']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_soln(soln):\n",
    "    \n",
    "    soln = [str(sol) for sol in soln]\n",
    "    soln = ''.join(soln)\n",
    "    \n",
    "    if soln in solns:\n",
    "        print('Solution is correct')\n",
    "    else:\n",
    "        print('Solution is wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "observations = 'HTTHTTHHTTHTTTHHTHHTTHTTTTHTHHTHTHHTTTH'\n",
    "observations_idxed = [0 if obsv == 'H' else 1 for obsv in observations]\n",
    "\n",
    "# Initial probabilities\n",
    "# [p_1 = 0.5, p_2 = 0.5]\n",
    "I = np.array([0.5, 0.5])\n",
    "\n",
    "# Transition Probabilities\n",
    "# [[state1->1, state1->2], [state2->1, state2->2]]\n",
    "A = np.array([[0.4, 0.6], [0.9, 0.1]])\n",
    "\n",
    "# Emission Probabilites\n",
    "# [[state1_head, state1_tail], [state2_head, state2_tail]]\n",
    "B = np.array([[0.49, 0.51], [0.85, 0.15]])\n",
    "\n",
    "# Array to store results\n",
    "res = np.zeros((len(observations,)), dtype = np.int32)\n",
    "\n",
    "# Number of possible states per observation\n",
    "N_states = I.shape[0]\n",
    "N_observations = len(observations)\n",
    "\n",
    "# numpy array to store probabilities\n",
    "probs = np.zeros((N_observations, N_states), dtype = np.float64)\n",
    "\n",
    "# array to store sequence of most probable states\n",
    "state_arr = []\n",
    "\n",
    "# Initialise initial probabilities\n",
    "first_obsv = observations_idxed[0]\n",
    "for j in range(N_states):\n",
    "    probs[0,j] = I[j]*B[j][first_obsv]\n",
    "\n",
    "# Mark the most probable path\n",
    "# +1 since probs is 0 indexed while the states are 1 indexed\n",
    "state_arr.append(np.argmax(probs[0,:]) + 1)\n",
    "\n",
    "# for 2nd observation to nth observation,\n",
    "for i in range(1, N_observations):\n",
    "        \n",
    "        # Get previous state\n",
    "        prev_state = state_arr[i-1] - 1\n",
    "        prev_prob = np.amax(probs[(i-1),:])\n",
    "        \n",
    "        # for each state, find out the probability\n",
    "        for j in range(N_states):\n",
    "            \n",
    "            probs[i,j] = B[j][observations_idxed[i]]*A[prev_state][j]*np.amax(prev_prob)\n",
    "            \n",
    "        # Find out most probable state given observation\n",
    "        current_state = np.argmax(probs[i,:])\n",
    "        \n",
    "        # Mark the most probable path\n",
    "        state_arr.append(current_state + 1)\n",
    "        \n",
    "# Return the most probable sequence of states\n",
    "print(state_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution is correct\n"
     ]
    }
   ],
   "source": [
    "test_soln(state_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "CS-431 Kernel",
   "language": "python",
   "name": "cs_431_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
